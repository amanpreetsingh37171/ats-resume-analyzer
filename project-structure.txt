# ATS-Resume-Analyzer — Streamlit + Python + ML

This repository provides a complete, runnable **ATS Resume Analyzer** built with **Streamlit** and a simple **machine learning** pipeline (TF-IDF + Logistic Regression). It supports uploading resumes (PDF, DOCX, TXT), extracting text, scoring against a job description, showing an "ATS score", highlighting matched skills/keywords, and a small training script to build a model from labeled data.

---

## Project structure

```
ats-resume-analyzer/
├── README.md
├── requirements.txt
├── sample_data.csv        # small sample dataset for training/demo
├── train_model.py         # script to train a TF-IDF + LogisticRegression model
├── app/
│   ├── streamlit_app.py   # main Streamlit app
│   ├── resume_parser.py   # functions to extract text from PDF/DOCX/TXT
│   └── helpers.py         # helper functions (scoring, highlighting)
├── models/
│   ├── tfidf.joblib       # saved vectorizer (created after training)
│   └── model.joblib       # saved classifier (created after training)
└── assets/
    └── example_resume.pdf
```

---

## Quick start

1. Create a Python virtual environment (recommended).

```bash
python -m venv venv
# windows
venv\Scripts\activate
# mac / linux
source venv/bin/activate
```

2. Install required packages

```bash
pip install -r requirements.txt
```

3. (Optional) Train the sample model using `sample_data.csv` (or use your labeled data):

```bash
python train_model.py --data sample_data.csv --outdir models
```

This will save `models/tfidf.joblib` and `models/model.joblib`.

4. Run the Streamlit app

```bash
streamlit run app/streamlit_app.py
```

Open the URL shown by Streamlit (usually `http://localhost:8501`).

---

## requirements.txt

```
streamlit
pandas
numpy
scikit-learn
joblib
pdfminer.six
python-docx
tqdm
pytest
```

---

## sample_data.csv (small demo)

```
text,label
"Experienced Python developer with scikit-learn, pandas, numpy",1
"Sales manager with 10 years in retail and CRM",0
"Machine learning engineer skilled in Python, TensorFlow, scikit-learn",1
"Graphic designer, Photoshop, Illustrator",0
"Data scientist with experience in NLP, scikit-learn, pandas",1
```

`label`: 1 = good fit / pass, 0 = not fit / fail. Replace or expand with your own labeled data for better results.

---

## train_model.py

```python
"""Train a simple TF-IDF + LogisticRegression pipeline and save the model and vectorizer.
Usage:
    python train_model.py --data sample_data.csv --outdir models
"""
import argparse
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import joblib
import os


def load_data(path):
    df = pd.read_csv(path)
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("CSV must have 'text' and 'label' columns")
    return df['text'].astype(str).tolist(), df['label'].astype(int).tolist()


def train(data_path, outdir, test_size=0.2, random_state=42):
    X, y = load_data(data_path)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=10000)),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    pipe.fit(X_train, y_train)

    y_pred = pipe.predict(X_test)
    y_proba = pipe.predict_proba(X_test)[:, 1]

    print(classification_report(y_test, y_pred))
    try:
        auc = roc_auc_score(y_test, y_proba)
        print(f"ROC AUC: {auc:.4f}")
    except Exception:
        pass

    os.makedirs(outdir, exist_ok=True)
    model_path = os.path.join(outdir, 'model.joblib')
    tfidf_path = os.path.join(outdir, 'tfidf.joblib')

    # save entire pipeline as model, and also save vectorizer separately
    joblib.dump(pipe, model_path)
    joblib.dump(pipe.named_steps['tfidf'], tfidf_path)

    print(f"Saved pipeline to {model_path}")
    print(f"Saved tfidf to {tfidf_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True, help='path to CSV with text,label columns')
    parser.add_argument('--outdir', default='models', help='output directory for model files')
    args = parser.parse_args()
    train(args.data, args.outdir)
```

---

## app/resume_parser.py

```python
"""Simple resume text extractor for PDF, DOCX, TXT.
Relies on pdfminer.six for PDFs and python-docx for DOCX.
"""
from io import StringIO
from pdfminer.high_level import extract_text
import docx


def extract_text_from_pdf(path_or_bytes):
    # path_or_bytes can be a path string; for uploaded file Streamlit provides file-like object
    if hasattr(path_or_bytes, 'read'):
        # file-like object
        data = path_or_bytes.read()
        if isinstance(data, bytes):
            # pdfminer can accept bytes via temporary file; write to temp
            import tempfile
            tf = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
            tf.write(data)
            tf.flush()
            tf.close()
            text = extract_text(tf.name)
            return text
        else:
            return str(data)
    else:
        return extract_text(path_or_bytes)


def extract_text_from_docx(path_or_file):
    if hasattr(path_or_file, 'read'):
        # python-docx expects a path; save temporary
        import tempfile
        data = path_or_file.read()
        tf = tempfile.NamedTemporaryFile(delete=False, suffix='.docx')
        tf.write(data)
        tf.flush()
        tf.close()
        doc = docx.Document(tf.name)
    else:
        doc = docx.Document(path_or_file)
    paragraphs = [p.text for p in doc.paragraphs]
    return '\n'.join(paragraphs)


def extract_text_from_txt(path_or_file):
    if hasattr(path_or_file, 'read'):
        data = path_or_file.read()
        if isinstance(data, bytes):
            try:
                return data.decode('utf-8', errors='ignore')
            except Exception:
                return str(data)
        return data
    else:
        with open(path_or_file, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()


def extract_text_from_file(uploaded_file):
    """Uploaded file is a streamlit UploadedFile-like object with .name and .read()."""
    name = uploaded_file.name.lower()
    if name.endswith('.pdf'):
        return extract_text_from_pdf(uploaded_file)
    if name.endswith('.docx') or name.endswith('.doc'):
        return extract_text_from_docx(uploaded_file)
    if name.endswith('.txt'):
        return extract_text_from_txt(uploaded_file)
    # fallback: read raw
    try:
        return uploaded_file.getvalue().decode('utf-8', errors='ignore')
    except Exception:
        return str(uploaded_file.read())
```

---

## app/helpers.py

```python
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter


def simple_keyword_score(resume_text, jd_text, top_k=30):
    """Return a simple keyword match score and matched keywords list.
    Score = matched_keywords_count / total_jd_keywords
    """
    # extract words from JD (basic tokenization)
    def tokens(text):
        words = re.findall(r"[a-zA-Z+#\d]+", text.lower())
        return [w for w in words if len(w) > 1]

    jd_tokens = tokens(jd_text)
    # pick top unique keywords from JD
    freq = Counter(jd_tokens)
    jd_keywords = [w for w,_ in freq.most_common(top_k)]

    resume_tokens = set(tokens(resume_text))
    matched = [kw for kw in jd_keywords if kw in resume_tokens]
    score = len(matched) / max(1, len(jd_keywords))
    return float(score), matched


def highlight_keywords(text, keywords):
    # naive highlight by wrapping tokens in <mark>
    def repl(match):
        w = match.group(0)
        if w.lower() in keywords:
            return f"<mark>{w}</mark>"
        return w
    pattern = re.compile(r"\b[\w+#]+\b", flags=re.IGNORECASE)
    return pattern.sub(repl, text)
```

---

## app/streamlit_app.py

```python
"""Streamlit front-end for ATS Resume Analyzer.
- Accepts job description (paste or upload)
- Upload single or multiple resumes
- Uses trained model (if available) to predict pass probability
- Also computes a simple keyword match score
"""
import streamlit as st
import joblib
import os
from resume_parser import extract_text_from_file
from helpers import simple_keyword_score, highlight_keywords

MODEL_PATH = os.path.join('..', 'models', 'model.joblib')
TFIDF_PATH = os.path.join('..', 'models', 'tfidf.joblib')
# allow local models in the app/models folder too
if os.path.exists(os.path.join('models', 'model.joblib')):
    MODEL_PATH = os.path.join('models', 'model.joblib')
    TFIDF_PATH = os.path.join('models', 'tfidf.joblib')

@st.cache_resource
def load_model(model_path=MODEL_PATH):
    if os.path.exists(model_path):
        try:
            m = joblib.load(model_path)
            return m
        except Exception as e:
            st.warning(f"Failed to load model: {e}")
            return None
    return None


@st.cache_data
def load_vectorizer(tfidf_path=TFIDF_PATH):
    if os.path.exists(tfidf_path):
        try:
            v = joblib.load(tfidf_path)
            return v
        except Exception as e:
            st.warning(f"Failed to load vectorizer: {e}")
            return None
    return None


st.set_page_config(page_title='ATS Resume Analyzer', layout='wide')
st.title('ATS Resume Analyzer')
st.write('Upload a job description and resumes; the app will score each resume for ATS fit.')

col1, col2 = st.columns([2,1])
with col1:
    jd_text = st.text_area('Paste job description here (or leave empty and upload a JD file)', height=250)
    jd_file = st.file_uploader('Or upload job description (.txt)', type=['txt'])
    if jd_file is not None and not jd_text.strip():
        jd_text = jd_file.getvalue().decode('utf-8', errors='ignore')

    uploaded_files = st.file_uploader('Upload resume(s) (PDF, DOCX, TXT)', accept_multiple_files=True)

with col2:
    st.info('Model: TF-IDF + LR (optional)')
    model = load_model()
    vectorizer = load_vectorizer()
    if model is None:
        st.warning('No trained model found. Upload `models/model.joblib` and `models/tfidf.joblib` in the project folder or run the training script.')
    st.write('---')
    st.write('Instructions:')
    st.write('- Provide a job description (paste or upload).')
    st.write('- Upload one or more resumes. The app will show ATS-like scores and suggestions.')

if uploaded_files:
    results = []
    for up in uploaded_files:
        text = extract_text_from_file(up)
        score_kw, matched = simple_keyword_score(text, jd_text or '')

        model_score = None
        if model is not None:
            try:
                # If model is a pipeline, it handles vectorization itself
                proba = model.predict_proba([jd_text + '\n' + text])[:, 1][0]
                model_score = float(proba)
            except Exception:
                # fallback: vectorize resume text only
                try:
                    vec = vectorizer.transform([text])
                    proba = model.predict_proba(vec)[:, 1][0]
                    model_score = float(proba)
                except Exception:
                    model_score = None

        # combine heuristic keyword score and model probability for a final ATS score
        if model_score is not None:
            final_score = 0.6 * model_score + 0.4 * score_kw
        else:
            final_score = score_kw

        results.append({
            'name': up.name,
            'text': text,
            'keyword_score': score_kw,
            'model_score': model_score,
            'final_score': final_score,
            'matched_keywords': matched
        })

    # Display results
    for r in results:
        st.header(r['name'])
        st.metric('ATS Score', f"{r['final_score']*100:.1f}%")
        c1, c2 = st.columns(2)
        with c1:
            st.subheader('Details')
            st.write(f"Keyword match: {len(r['matched_keywords'])} - {', '.join(r['matched_keywords'][:20])}")
            if r['model_score'] is not None:
                st.write(f"Model probability (pass): {r['model_score']:.3f}")
            st.write('---')
            st.download_button('Download extracted text', data=r['text'], file_name=f"{r['name']}.txt")
        with c2:
            st.subheader('Resume (highlighted)')
            highlighted = highlight_keywords(r['text'][:10000], [k.lower() for k in r['matched_keywords']])
            st.markdown(highlighted.replace('\n', '  \n'), unsafe_allow_html=True)

else:
    st.info('No resumes uploaded yet.')

# Optional: allow batch scoring from a folder or zipped resumes
```

---

## Notes & next steps

* The provided ML pipeline is intentionally simple (TF-IDF + LogisticRegression) so it's easy to train and understand. For a production-ready system consider:

  * Using stronger models (fine-tuned transformers, embeddings + similarity)
  * Better resume parsing (NLP entity extraction, section detection)
  * A labeled dataset with real resumes and job descriptions
  * More refined scoring (weighted skill matches, section importance)

* If you saw `pyresparser` errors earlier, this scaffold purposely avoids `pyresparser`. Instead it uses `pdfminer.six` and `python-docx` which are more controllable.

---

If you want, I can:

* Create a ZIP with the full project files.
* Replace the simple ML model with an embeddings-based similarity approach.
* Add a sample Streamlit layout with nicer UI (cards, progress bars).

Tell me which one you'd like next and I will update the project files accordingly.
